# Session 2025-09-14

Scope: Progressive search — best-candidate selection (coverage-based)

- 09:10 Notes: Algorithm for selecting the best highlight when multiple candidates exist
  Output:
  ```text
  Problem:
    Shortest-range selection can pick snippets that are too short and miss
    important query words. We want a quality-aware selection.

  Inputs:
    - Page-level normalized text (from _build_page_text_and_map)
    - Query text
    - Progressive candidates: Each candidate is a list of segments produced by
      progressive search. A segment is (start, end, k_used), where k_used is
      the number of query words matched in that chunk.

  Candidate generation:
    - For k0 in [kmax..1], find all occurrences of the first chunk and greedily
      chain 3→2→1-word chunks left-to-right (prev_end-based), tolerating \s*.
    - Accept a candidate if:
        * Every inter-segment gap ≤ max_segment_gap_chars
        * Total matched words Σk_used ≥ min_total_words (default 4)

  Scoring metrics per candidate:
    - word_coverage   = matched_words / total_query_words
    - char_coverage   = matched_chars / range_len
    - gap_penalty     = gap_sum / range_len
    - length_penalty  = min(1.0, range_len / 800.0)  # gentle penalty

  Score (higher is better):
    score = 2.0 * word_coverage
            + 1.0 * char_coverage
            - 0.5 * gap_penalty
            - 0.25 * length_penalty

  Selection policy:
    - Default (shortest mode in CLI terms): pick the single best-scoring
      candidate globally across pages; tie-break by shorter length, then
      earlier start.
    - All matches: return all candidates (per page), no scoring selection.

  Rationale:
    - Prioritize candidates covering more of the requested words and with less
      non-matching filler, while still slightly preferring shorter spans.
  ```
  Why: Record the selection logic and scoring for future reference

- 09:18 Result: Implemented best-candidate selection and wired it into
  progressive search; default single selection now uses coverage-based scoring.

- 09:30 Notes: Unicode-kind tokenization for queries (CJK/ASCII agnostic)
  Output:
  ```text
  Goal:
    Make progressive search robust for Japanese (no spaces) and mixed scripts
    without distinguishing CJK vs ASCII explicitly.

  Tokenization (query side):
    - Normalize query (NFKC, dash/quote unify, spaces→' ').
    - Drop whitespace entirely.
    - Classify characters by Unicode General Category:
        'W' = word-like: Letters (L*), Numbers (N*), Marks (M*)
        'D' = delimiters: Punctuation (P*), Symbols (S*)
        'O' = other: Controls/Separators (C*, Z* except spaces)
    - Build tokens as:
        * contiguous runs of 'W' become one token
        * each 'D' is its own 1-char token
        * 'O' is skipped
    - Result: boundary by character kind; spaces excluded; delimiters are
      single-char tokens.

  Matching:
    - Between tokens, pattern uses \s* allowing line breaks/zero-width joins.
    - Progressive chunks still 3→2→1 tokens; scoring selection unchanged.
  ```
  Why: Record the CJK/ASCII-agnostic tokenization rule used by progressive search
